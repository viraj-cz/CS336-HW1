{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c915377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import regex as re\n",
    "from einops import rearrange, reduce, repeat\n",
    "import os\n",
    "from multiprocessing import Process, Lock, Pool\n",
    "from collections import defaultdict, Counter\n",
    "from pretokenization_example import find_chunk_boundaries\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(args):\n",
    "        input_path, start, end, special_tokens = args\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            f.seek(start)\n",
    "            pre_chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            if special_tokens:\n",
    "                pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "                pre_chunk = [c for c in re.split(pattern, pre_chunk) if c]\n",
    "            else:\n",
    "                pre_chunk = [pre_chunk]\n",
    "            \n",
    "        PAT = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        parts = []\n",
    "        for c in pre_chunk:\n",
    "            for tok in PAT.findall(c): #replace this with finditer later [LATER]\n",
    "                parts.append(tuple(bytes([b]) for b in tok.encode(\"utf8\"))) \n",
    "        \n",
    "        return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    max_loops=None,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    # use multi-processing and chunks [LATER]\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    curr_vocab_size = 256\n",
    "    for st in special_tokens:\n",
    "        vocab[curr_vocab_size] = st.encode(\"utf-8\")\n",
    "        curr_vocab_size += 1\n",
    "    \n",
    "    merges = []\n",
    "     \n",
    "    args_list = []\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        num_processes = os.cpu_count() or 1\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            args_list.append((input_path, start, end, special_tokens))\n",
    "\n",
    "    #CHECK [LATER]\n",
    "    num_chunks = max(0, len(args_list))\n",
    "    num_processes = min(num_processes, max(1, num_chunks))\n",
    "\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(worker, args_list)\n",
    "    \n",
    "    parts = list(chain.from_iterable(results)) #check [LATER]\n",
    "                \n",
    "    loop_counter = 0\n",
    "    total = (vocab_size - curr_vocab_size) if max_loops is None else min(vocab_size - curr_vocab_size, max_loops)\n",
    "    pbar = tqdm(total=total, desc=\"BPE\")\n",
    "\n",
    "    while (max_loops is None or loop_counter < max_loops ) and (curr_vocab_size < vocab_size):\n",
    "        merge_dict = defaultdict(int)\n",
    "        for part in parts:\n",
    "            total_letters = len(part)\n",
    "            k = 0\n",
    "            while (k+1) < total_letters:\n",
    "                merge_dict[(part[k], part[k+1])] += 1\n",
    "                k +=1\n",
    "\n",
    "        if not merge_dict:\n",
    "            break\n",
    "        new_token = max(merge_dict.items(), key=lambda x: (x[1], x[0]))[0] #max is EMPTY [TODO]\n",
    "        merges.append((new_token[0], new_token[1]))\n",
    "        new_token = new_token[0] + new_token[1]\n",
    "        vocab[curr_vocab_size] = new_token\n",
    "        curr_vocab_size += 1\n",
    "\n",
    "        def apply_merge(part):\n",
    "            j = 0\n",
    "            out = []\n",
    "            while j+1 < len(part):\n",
    "                if (part[j] + part[j+1]) == new_token:\n",
    "                    out.append(new_token)\n",
    "                    j += 2\n",
    "                else:\n",
    "                    out.append(part[j])\n",
    "                    j += 1\n",
    "                \n",
    "            if j < len(part):\n",
    "                out.append(part[j])\n",
    "            return tuple(out)\n",
    "\n",
    "        new_parts = [apply_merge(p) for p in parts]\n",
    "        if new_parts == parts:\n",
    "            break\n",
    "        parts = new_parts\n",
    "        \n",
    "        pbar.update(1)\n",
    "        loop_counter+= 1\n",
    "        \n",
    "    pbar.close()\n",
    "    return (vocab, merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36497cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_train_bpe(\n",
    "    \"/Users/virajchhajed/Desktop/everything/fun/cs336/hw-1/data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    1000,\n",
    "    [\"<|endoftext|>\"],\n",
    "    1000\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336 (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
