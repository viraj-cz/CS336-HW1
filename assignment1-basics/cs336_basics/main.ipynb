{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c915377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import regex as re\n",
    "from einops import rearrange, reduce, repeat\n",
    "import os\n",
    "import multiprocessing\n",
    "from collections import defaultdict, Counter\n",
    "from pretokenization_example import find_chunk_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b3ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    # use multi-processing and chunks [LATER]\n",
    "\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    curr_vocab_size = 256\n",
    "    for st in special_tokens:\n",
    "        vocab[curr_vocab_size] = st.encode(\"utf-8\")\n",
    "        curr_vocab_size += 1\n",
    "    \n",
    "    merges = []\n",
    "\n",
    "    while curr_vocab_size < vocab_size:\n",
    "\n",
    "        # The Regex Pattern for Pre-tokenization\n",
    "        PAT = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "        # Think about whether you'd like to replace with a streamed/buffed implementation\n",
    "        fd = os.open(input_path, os.O_RDONLY)\n",
    "        data = os.read(fd, os.path.getsize(input_path))\n",
    "        os.close(fd)\n",
    "        text = data.decode(\"utf-8\")\n",
    "\n",
    "        #gotta strip the text of the end of text symbol here [LATER]\n",
    "\n",
    "        #replace this with finditer later [LATER]\n",
    "        parts = re.findall(PAT, text)\n",
    "        # parts = re.findall(r\"\\S+\", test_corpus)\n",
    "        parts = [tuple(i) for i in parts]\n",
    "\n",
    "    \n",
    "        for i in range(6):\n",
    "\n",
    "            merge_dict = defaultdict(int)\n",
    "            for part in parts:\n",
    "                total_letters = len(part)\n",
    "                k = 0\n",
    "                while (k+1) < total_letters:\n",
    "                    merge_dict[(part[k], part[k+1])] += 1\n",
    "                    k +=1\n",
    "\n",
    "            #lexiographically higherst occuring one:\n",
    "            new_token = max(merge_dict.items(), key=lambda x: (x[1], x[0]))[0]\n",
    "            merges.append(new_token)\n",
    "            new_token = ''.join(new_token)\n",
    "            vocab[curr_vocab_size] = new_token.encode(\"utf8\")\n",
    "            curr_vocab_size += 1\n",
    "\n",
    "            # continue from here\n",
    "            to_be_removed = []\n",
    "            to_be_added = []\n",
    "            for i in parts:\n",
    "                lenght = len(i)\n",
    "                j = 0\n",
    "                while (j+1) < lenght:\n",
    "                    if (i[j] + i[j+1]) == new_token:\n",
    "                        to_be_removed.append(i)\n",
    "                        new_i = i[:j] + (new_token,) + i[j+2:]\n",
    "                        to_be_added.append(new_i)\n",
    "                    j+= 1\n",
    "\n",
    "            for i in to_be_removed:\n",
    "                parts.remove(i)\n",
    "            parts.extend(to_be_added)\n",
    "            \n",
    "        break\n",
    "\n",
    "    return (vocab, merges)\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36497cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train_bpe(\n",
    "    \"/Users/virajchhajed/Desktop/everything/fun/cs336/hw-1/data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    100_000,\n",
    "    [\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bc99e",
   "metadata": {},
   "source": [
    "- how does multiprocessing work?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336 (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
