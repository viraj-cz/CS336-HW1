{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c915377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import regex as re\n",
    "from einops import rearrange, reduce, repeat\n",
    "import os\n",
    "import multiprocessing\n",
    "from collections import defaultdict, Counter\n",
    "from pretokenization_example import find_chunk_boundaries\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    max_loops=None,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    # use multi-processing and chunks [LATER]\n",
    "\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    curr_vocab_size = 256\n",
    "    for st in special_tokens:\n",
    "        vocab[curr_vocab_size] = st.encode(\"utf-8\")\n",
    "        curr_vocab_size += 1\n",
    "    \n",
    "    merges = []\n",
    "\n",
    "     # The Regex Pattern for Pre-tokenization\n",
    "    PAT = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    # Think about whether you'd like to replace with a streamed/buffed implementation\n",
    "    fd = os.open(input_path, os.O_RDONLY)\n",
    "    data = os.read(fd, os.path.getsize(input_path))\n",
    "    os.close(fd)\n",
    "    text = data.decode(\"utf-8\")\n",
    "\n",
    "    if special_tokens:\n",
    "        pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "        chunks = [c for c in re.split(pattern, text) if c]\n",
    "    else:\n",
    "        chunks = [text]\n",
    "\n",
    "    #replace this with finditer later [LATER]\n",
    "    parts = []\n",
    "    for c in chunks:\n",
    "        for tok in re.findall(PAT, c):\n",
    "            parts.append(tuple(bytes([b]) for b in tok.encode(\"utf8\"))) \n",
    "\n",
    "    loop_counter = 0\n",
    "    total = (vocab_size - curr_vocab_size) if max_loops is None else min(vocab_size - curr_vocab_size, max_loops)\n",
    "    pbar = tqdm(total=total, desc=\"BPE\")\n",
    "\n",
    "    while (max_loops is None or loop_counter < max_loops ) and (curr_vocab_size < vocab_size):\n",
    "        merge_dict = defaultdict(int)\n",
    "        for part in parts:\n",
    "            total_letters = len(part)\n",
    "            k = 0\n",
    "            while (k+1) < total_letters:\n",
    "                merge_dict[(part[k], part[k+1])] += 1\n",
    "                k +=1\n",
    "\n",
    "        if not merge_dict:\n",
    "            break\n",
    "        new_token = max(merge_dict.items(), key=lambda x: (x[1], x[0]))[0] #max is EMPTY [TODO]\n",
    "        merges.append((new_token[0], new_token[1]))\n",
    "        new_token = new_token[0] + new_token[1]\n",
    "        vocab[curr_vocab_size] = new_token\n",
    "        curr_vocab_size += 1\n",
    "\n",
    "        def apply_merge(part):\n",
    "            j = 0\n",
    "            out = []\n",
    "            while j+1 < len(part):\n",
    "                if (part[j] + part[j+1]) == new_token:\n",
    "                    out.append(new_token)\n",
    "                    j += 2\n",
    "                else:\n",
    "                    out.append(part[j])\n",
    "                    j += 1\n",
    "                \n",
    "            if j < len(part):\n",
    "                out.append(part[j])\n",
    "            return tuple(out)\n",
    "\n",
    "        new_parts = [apply_merge(p) for p in parts]\n",
    "        if new_parts == parts:\n",
    "            break\n",
    "        parts = new_parts\n",
    "        \n",
    "        pbar.update(1)\n",
    "        loop_counter+= 1\n",
    "        \n",
    "    pbar.close()\n",
    "    return (vocab, merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36497cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train_bpe(\n",
    "    \"/Users/virajchhajed/Desktop/everything/fun/cs336/hw-1/data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    100_000,\n",
    "    [\"<|endoftext|>\"],\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254a4135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "b'abc'\n",
      "def\n",
      "b'def'\n",
      "ðŸŸ¥ðŸ˜‚\n",
      "b'\\xf0\\x9f\\x9f\\xa5\\xf0\\x9f\\x98\\x82'\n"
     ]
    }
   ],
   "source": [
    "chunks = [\"abc\", \"defðŸŸ¥ðŸ˜‚\"]\n",
    "\n",
    "PAT = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "parts = []\n",
    "\n",
    "for c in chunks:\n",
    "        for tok in re.findall(PAT, c):\n",
    "            print(tok)\n",
    "            print(tok.encode(\"utf8\"))\n",
    "            parts.append(tuple(bytes([b]) for b in tok.encode(\"utf8\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad34687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'a', b'b', b'c'),\n",
       " (b'd', b'e', b'f'),\n",
       " (b'\\xf0', b'\\x9f', b'\\x9f', b'\\xa5', b'\\xf0', b'\\x9f', b'\\x98', b'\\x82')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42171c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a2313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xf0'\n",
      "b'\\x9f'\n",
      "b'\\x98'\n",
      "b'\\x82'\n"
     ]
    }
   ],
   "source": [
    "for i in \"ðŸ˜‚\".encode(\"utf8\"):\n",
    "    print(bytes([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7167e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336 (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
