{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c915377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import regex as re\n",
    "from einops import rearrange, reduce, repeat\n",
    "import os\n",
    "import multiprocessing\n",
    "from collections import defaultdict, Counter\n",
    "from pretokenization_example import find_chunk_boundaries\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f853d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = '''low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    max_loops=None,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    # use multi-processing and chunks [LATER]\n",
    "\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    curr_vocab_size = 256\n",
    "    for st in special_tokens:\n",
    "        vocab[curr_vocab_size] = st.encode(\"utf-8\")\n",
    "        curr_vocab_size += 1\n",
    "    \n",
    "    merges = []\n",
    "\n",
    "     # The Regex Pattern for Pre-tokenization\n",
    "    PAT = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    # Think about whether you'd like to replace with a streamed/buffed implementation\n",
    "    fd = os.open(input_path, os.O_RDONLY)\n",
    "    data = os.read(fd, os.path.getsize(input_path))\n",
    "    os.close(fd)\n",
    "    text = data.decode(\"utf-8\")\n",
    "\n",
    "    #gotta strip the text of the end of text symbol here [LATER]\n",
    "\n",
    "    #replace this with finditer later [LATER]\n",
    "    # parts = re.findall(PAT, text)\n",
    "    parts = re.findall(r\"\\S+\", test_corpus)\n",
    "    parts = [tuple(i) for i in parts]\n",
    "\n",
    "    loop_counter = 0\n",
    "    pbar = tqdm(desc=\"BPE\")\n",
    "    while (max_loops is None or loop_counter < max_loops ) and (curr_vocab_size < vocab_size):\n",
    "        merge_dict = defaultdict(int)\n",
    "        for part in parts:\n",
    "            total_letters = len(part)\n",
    "            k = 0\n",
    "            while (k+1) < total_letters:\n",
    "                merge_dict[(part[k], part[k+1])] += 1\n",
    "                k +=1\n",
    "\n",
    "        #lexiographically higherst occuring one:\n",
    "        if not merge_dict:\n",
    "            break\n",
    "        new_token = max(merge_dict.items(), key=lambda x: (x[1], x[0]))[0] #max is EMPTY [TODO]\n",
    "        merges.append(new_token)\n",
    "        new_token = ''.join(new_token)\n",
    "        vocab[curr_vocab_size] = new_token.encode(\"utf8\")\n",
    "        curr_vocab_size += 1\n",
    "\n",
    "        # continue from here\n",
    "        to_be_removed = []\n",
    "        to_be_added = []\n",
    "        for i in parts:\n",
    "            lenght = len(i)\n",
    "            j = 0\n",
    "            while (j+1) < lenght:\n",
    "                if (i[j] + i[j+1]) == new_token:\n",
    "                    to_be_removed.append(i)\n",
    "                    new_i = i[:j] + (new_token,) + i[j+2:]\n",
    "                    to_be_added.append(new_i)\n",
    "                j+= 1\n",
    "\n",
    "        for i in to_be_removed:\n",
    "            parts.remove(i)\n",
    "        parts.extend(to_be_added)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        loop_counter+= 1\n",
    "        \n",
    "    pbar.close()\n",
    "    return (vocab, merges)\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36497cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BPE: 12it [00:00, 17829.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w', 'e', 'r'), ('l', 'o', 'w', 'e', 'r'), ('w', 'i', 'd', 'e', 's', 't'), ('w', 'i', 'd', 'e', 's', 't'), ('w', 'i', 'd', 'e', 's', 't'), ('n', 'e', 'w', 'e', 's', 't'), ('n', 'e', 'w', 'e', 's', 't'), ('n', 'e', 'w', 'e', 's', 't'), ('n', 'e', 'w', 'e', 's', 't'), ('n', 'e', 'w', 'e', 's', 't'), ('n', 'e', 'w', 'e', 's', 't')]\n",
      "defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', 'e'): 8, ('e', 'r'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3, ('e', 's'): 9, ('s', 't'): 9, ('n', 'e'): 6, ('e', 'w'): 6})\n",
      "[('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w', 'e', 'r'), ('l', 'o', 'w', 'e', 'r'), ('w', 'i', 'd', 'e', 'st'), ('w', 'i', 'd', 'e', 'st'), ('w', 'i', 'd', 'e', 'st'), ('n', 'e', 'w', 'e', 'st'), ('n', 'e', 'w', 'e', 'st'), ('n', 'e', 'w', 'e', 'st'), ('n', 'e', 'w', 'e', 'st'), ('n', 'e', 'w', 'e', 'st'), ('n', 'e', 'w', 'e', 'st')]\n",
      "defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', 'e'): 8, ('e', 'r'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3, ('e', 'st'): 9, ('n', 'e'): 6, ('e', 'w'): 6})\n",
      "[('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w'), ('l', 'o', 'w', 'e', 'r'), ('l', 'o', 'w', 'e', 'r'), ('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est')]\n",
      "defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', 'e'): 2, ('e', 'r'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6})\n",
      "[('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('l', 'ow'), ('l', 'ow'), ('l', 'ow'), ('l', 'ow'), ('l', 'ow'), ('l', 'ow', 'e', 'r'), ('l', 'ow', 'e', 'r')]\n",
      "defaultdict(<class 'int'>, {('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('l', 'ow'): 7, ('ow', 'e'): 2, ('e', 'r'): 2})\n",
      "[('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('n', 'e', 'w', 'est'), ('low',), ('low',), ('low',), ('low',), ('low',), ('low', 'e', 'r'), ('low', 'e', 'r')]\n",
      "defaultdict(<class 'int'>, {('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('low', 'e'): 2, ('e', 'r'): 2})\n",
      "[('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('low',), ('low',), ('low',), ('low',), ('low',), ('low', 'e', 'r'), ('low', 'e', 'r'), ('n', 'e', 'west'), ('n', 'e', 'west'), ('n', 'e', 'west'), ('n', 'e', 'west'), ('n', 'e', 'west'), ('n', 'e', 'west')]\n",
      "defaultdict(<class 'int'>, {('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3, ('low', 'e'): 2, ('e', 'r'): 2, ('n', 'e'): 6, ('e', 'west'): 6})\n",
      "[('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('low',), ('low',), ('low',), ('low',), ('low',), ('low', 'e', 'r'), ('low', 'e', 'r'), ('ne', 'west'), ('ne', 'west'), ('ne', 'west'), ('ne', 'west'), ('ne', 'west'), ('ne', 'west')]\n",
      "defaultdict(<class 'int'>, {('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3, ('low', 'e'): 2, ('e', 'r'): 2, ('ne', 'west'): 6})\n",
      "[('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('w', 'i', 'd', 'est'), ('low',), ('low',), ('low',), ('low',), ('low',), ('low', 'e', 'r'), ('low', 'e', 'r'), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',)]\n",
      "defaultdict(<class 'int'>, {('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3, ('low', 'e'): 2, ('e', 'r'): 2})\n",
      "[('low',), ('low',), ('low',), ('low',), ('low',), ('low', 'e', 'r'), ('low', 'e', 'r'), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('wi', 'd', 'est'), ('wi', 'd', 'est'), ('wi', 'd', 'est')]\n",
      "defaultdict(<class 'int'>, {('low', 'e'): 2, ('e', 'r'): 2, ('wi', 'd'): 3, ('d', 'est'): 3})\n",
      "[('low',), ('low',), ('low',), ('low',), ('low',), ('low', 'e', 'r'), ('low', 'e', 'r'), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('wid', 'est'), ('wid', 'est'), ('wid', 'est')]\n",
      "defaultdict(<class 'int'>, {('low', 'e'): 2, ('e', 'r'): 2, ('wid', 'est'): 3})\n",
      "[('low',), ('low',), ('low',), ('low',), ('low',), ('low', 'e', 'r'), ('low', 'e', 'r'), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('widest',), ('widest',), ('widest',)]\n",
      "defaultdict(<class 'int'>, {('low', 'e'): 2, ('e', 'r'): 2})\n",
      "[('low',), ('low',), ('low',), ('low',), ('low',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('widest',), ('widest',), ('widest',), ('lowe', 'r'), ('lowe', 'r')]\n",
      "defaultdict(<class 'int'>, {('lowe', 'r'): 2})\n",
      "[('low',), ('low',), ('low',), ('low',), ('low',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('newest',), ('widest',), ('widest',), ('widest',), ('lower',), ('lower',)]\n",
      "defaultdict(<class 'int'>, {})\n",
      "broken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: b'\\x00',\n",
       "  1: b'\\x01',\n",
       "  2: b'\\x02',\n",
       "  3: b'\\x03',\n",
       "  4: b'\\x04',\n",
       "  5: b'\\x05',\n",
       "  6: b'\\x06',\n",
       "  7: b'\\x07',\n",
       "  8: b'\\x08',\n",
       "  9: b'\\t',\n",
       "  10: b'\\n',\n",
       "  11: b'\\x0b',\n",
       "  12: b'\\x0c',\n",
       "  13: b'\\r',\n",
       "  14: b'\\x0e',\n",
       "  15: b'\\x0f',\n",
       "  16: b'\\x10',\n",
       "  17: b'\\x11',\n",
       "  18: b'\\x12',\n",
       "  19: b'\\x13',\n",
       "  20: b'\\x14',\n",
       "  21: b'\\x15',\n",
       "  22: b'\\x16',\n",
       "  23: b'\\x17',\n",
       "  24: b'\\x18',\n",
       "  25: b'\\x19',\n",
       "  26: b'\\x1a',\n",
       "  27: b'\\x1b',\n",
       "  28: b'\\x1c',\n",
       "  29: b'\\x1d',\n",
       "  30: b'\\x1e',\n",
       "  31: b'\\x1f',\n",
       "  32: b' ',\n",
       "  33: b'!',\n",
       "  34: b'\"',\n",
       "  35: b'#',\n",
       "  36: b'$',\n",
       "  37: b'%',\n",
       "  38: b'&',\n",
       "  39: b\"'\",\n",
       "  40: b'(',\n",
       "  41: b')',\n",
       "  42: b'*',\n",
       "  43: b'+',\n",
       "  44: b',',\n",
       "  45: b'-',\n",
       "  46: b'.',\n",
       "  47: b'/',\n",
       "  48: b'0',\n",
       "  49: b'1',\n",
       "  50: b'2',\n",
       "  51: b'3',\n",
       "  52: b'4',\n",
       "  53: b'5',\n",
       "  54: b'6',\n",
       "  55: b'7',\n",
       "  56: b'8',\n",
       "  57: b'9',\n",
       "  58: b':',\n",
       "  59: b';',\n",
       "  60: b'<',\n",
       "  61: b'=',\n",
       "  62: b'>',\n",
       "  63: b'?',\n",
       "  64: b'@',\n",
       "  65: b'A',\n",
       "  66: b'B',\n",
       "  67: b'C',\n",
       "  68: b'D',\n",
       "  69: b'E',\n",
       "  70: b'F',\n",
       "  71: b'G',\n",
       "  72: b'H',\n",
       "  73: b'I',\n",
       "  74: b'J',\n",
       "  75: b'K',\n",
       "  76: b'L',\n",
       "  77: b'M',\n",
       "  78: b'N',\n",
       "  79: b'O',\n",
       "  80: b'P',\n",
       "  81: b'Q',\n",
       "  82: b'R',\n",
       "  83: b'S',\n",
       "  84: b'T',\n",
       "  85: b'U',\n",
       "  86: b'V',\n",
       "  87: b'W',\n",
       "  88: b'X',\n",
       "  89: b'Y',\n",
       "  90: b'Z',\n",
       "  91: b'[',\n",
       "  92: b'\\\\',\n",
       "  93: b']',\n",
       "  94: b'^',\n",
       "  95: b'_',\n",
       "  96: b'`',\n",
       "  97: b'a',\n",
       "  98: b'b',\n",
       "  99: b'c',\n",
       "  100: b'd',\n",
       "  101: b'e',\n",
       "  102: b'f',\n",
       "  103: b'g',\n",
       "  104: b'h',\n",
       "  105: b'i',\n",
       "  106: b'j',\n",
       "  107: b'k',\n",
       "  108: b'l',\n",
       "  109: b'm',\n",
       "  110: b'n',\n",
       "  111: b'o',\n",
       "  112: b'p',\n",
       "  113: b'q',\n",
       "  114: b'r',\n",
       "  115: b's',\n",
       "  116: b't',\n",
       "  117: b'u',\n",
       "  118: b'v',\n",
       "  119: b'w',\n",
       "  120: b'x',\n",
       "  121: b'y',\n",
       "  122: b'z',\n",
       "  123: b'{',\n",
       "  124: b'|',\n",
       "  125: b'}',\n",
       "  126: b'~',\n",
       "  127: b'\\x7f',\n",
       "  128: b'\\x80',\n",
       "  129: b'\\x81',\n",
       "  130: b'\\x82',\n",
       "  131: b'\\x83',\n",
       "  132: b'\\x84',\n",
       "  133: b'\\x85',\n",
       "  134: b'\\x86',\n",
       "  135: b'\\x87',\n",
       "  136: b'\\x88',\n",
       "  137: b'\\x89',\n",
       "  138: b'\\x8a',\n",
       "  139: b'\\x8b',\n",
       "  140: b'\\x8c',\n",
       "  141: b'\\x8d',\n",
       "  142: b'\\x8e',\n",
       "  143: b'\\x8f',\n",
       "  144: b'\\x90',\n",
       "  145: b'\\x91',\n",
       "  146: b'\\x92',\n",
       "  147: b'\\x93',\n",
       "  148: b'\\x94',\n",
       "  149: b'\\x95',\n",
       "  150: b'\\x96',\n",
       "  151: b'\\x97',\n",
       "  152: b'\\x98',\n",
       "  153: b'\\x99',\n",
       "  154: b'\\x9a',\n",
       "  155: b'\\x9b',\n",
       "  156: b'\\x9c',\n",
       "  157: b'\\x9d',\n",
       "  158: b'\\x9e',\n",
       "  159: b'\\x9f',\n",
       "  160: b'\\xa0',\n",
       "  161: b'\\xa1',\n",
       "  162: b'\\xa2',\n",
       "  163: b'\\xa3',\n",
       "  164: b'\\xa4',\n",
       "  165: b'\\xa5',\n",
       "  166: b'\\xa6',\n",
       "  167: b'\\xa7',\n",
       "  168: b'\\xa8',\n",
       "  169: b'\\xa9',\n",
       "  170: b'\\xaa',\n",
       "  171: b'\\xab',\n",
       "  172: b'\\xac',\n",
       "  173: b'\\xad',\n",
       "  174: b'\\xae',\n",
       "  175: b'\\xaf',\n",
       "  176: b'\\xb0',\n",
       "  177: b'\\xb1',\n",
       "  178: b'\\xb2',\n",
       "  179: b'\\xb3',\n",
       "  180: b'\\xb4',\n",
       "  181: b'\\xb5',\n",
       "  182: b'\\xb6',\n",
       "  183: b'\\xb7',\n",
       "  184: b'\\xb8',\n",
       "  185: b'\\xb9',\n",
       "  186: b'\\xba',\n",
       "  187: b'\\xbb',\n",
       "  188: b'\\xbc',\n",
       "  189: b'\\xbd',\n",
       "  190: b'\\xbe',\n",
       "  191: b'\\xbf',\n",
       "  192: b'\\xc0',\n",
       "  193: b'\\xc1',\n",
       "  194: b'\\xc2',\n",
       "  195: b'\\xc3',\n",
       "  196: b'\\xc4',\n",
       "  197: b'\\xc5',\n",
       "  198: b'\\xc6',\n",
       "  199: b'\\xc7',\n",
       "  200: b'\\xc8',\n",
       "  201: b'\\xc9',\n",
       "  202: b'\\xca',\n",
       "  203: b'\\xcb',\n",
       "  204: b'\\xcc',\n",
       "  205: b'\\xcd',\n",
       "  206: b'\\xce',\n",
       "  207: b'\\xcf',\n",
       "  208: b'\\xd0',\n",
       "  209: b'\\xd1',\n",
       "  210: b'\\xd2',\n",
       "  211: b'\\xd3',\n",
       "  212: b'\\xd4',\n",
       "  213: b'\\xd5',\n",
       "  214: b'\\xd6',\n",
       "  215: b'\\xd7',\n",
       "  216: b'\\xd8',\n",
       "  217: b'\\xd9',\n",
       "  218: b'\\xda',\n",
       "  219: b'\\xdb',\n",
       "  220: b'\\xdc',\n",
       "  221: b'\\xdd',\n",
       "  222: b'\\xde',\n",
       "  223: b'\\xdf',\n",
       "  224: b'\\xe0',\n",
       "  225: b'\\xe1',\n",
       "  226: b'\\xe2',\n",
       "  227: b'\\xe3',\n",
       "  228: b'\\xe4',\n",
       "  229: b'\\xe5',\n",
       "  230: b'\\xe6',\n",
       "  231: b'\\xe7',\n",
       "  232: b'\\xe8',\n",
       "  233: b'\\xe9',\n",
       "  234: b'\\xea',\n",
       "  235: b'\\xeb',\n",
       "  236: b'\\xec',\n",
       "  237: b'\\xed',\n",
       "  238: b'\\xee',\n",
       "  239: b'\\xef',\n",
       "  240: b'\\xf0',\n",
       "  241: b'\\xf1',\n",
       "  242: b'\\xf2',\n",
       "  243: b'\\xf3',\n",
       "  244: b'\\xf4',\n",
       "  245: b'\\xf5',\n",
       "  246: b'\\xf6',\n",
       "  247: b'\\xf7',\n",
       "  248: b'\\xf8',\n",
       "  249: b'\\xf9',\n",
       "  250: b'\\xfa',\n",
       "  251: b'\\xfb',\n",
       "  252: b'\\xfc',\n",
       "  253: b'\\xfd',\n",
       "  254: b'\\xfe',\n",
       "  255: b'\\xff',\n",
       "  256: b'<|endoftext|>',\n",
       "  257: b'st',\n",
       "  258: b'est',\n",
       "  259: b'ow',\n",
       "  260: b'low',\n",
       "  261: b'west',\n",
       "  262: b'ne',\n",
       "  263: b'newest',\n",
       "  264: b'wi',\n",
       "  265: b'wid',\n",
       "  266: b'widest',\n",
       "  267: b'lowe',\n",
       "  268: b'lower'},\n",
       " [('s', 't'),\n",
       "  ('e', 'st'),\n",
       "  ('o', 'w'),\n",
       "  ('l', 'ow'),\n",
       "  ('w', 'est'),\n",
       "  ('n', 'e'),\n",
       "  ('ne', 'west'),\n",
       "  ('w', 'i'),\n",
       "  ('wi', 'd'),\n",
       "  ('wid', 'est'),\n",
       "  ('low', 'e'),\n",
       "  ('lowe', 'r')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train_bpe(\n",
    "    \"/Users/virajchhajed/Desktop/everything/fun/cs336/hw-1/data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    100_000,\n",
    "    [\"<|endoftext|>\"],\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bc99e",
   "metadata": {},
   "source": [
    "- how does multiprocessing work?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336 (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
